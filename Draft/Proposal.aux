\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{Lasso}
\citation{ElasticNet}
\citation{NonlinearEqLInfty}
\citation{CharalambousC}
\citation{distributedcontrol}
\citation{lowcomplexity}
\citation{phaseretrieval}
\citation{NonlinearEqLp}
\citation{IoffeAD1979}
\citation{BerkeJV1987}
\citation{Lagrange}
\citation{comp21,comp22,comp56}
\citation{proximal,accproximal}
\citation{ADMM}
\citation{MM}
\citation{proximalauglag}
\citation{bundle,bundle2,bundle3}
\citation{bundlefree1,bundlefree2}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{problem}{{1}{1}{Introduction}{equation.1.1}{}}
\citation{comp55}
\citation{comp44,comp56}
\citation{manifoldl1}
\citation{grasam1,grasam2}
\citation{manifoldpiecewiselinear}
\citation{manifold}
\citation{manifold}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Problem Setting}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Notations}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Definitions and Assumptions}{2}{subsection.2.3}\protected@file@percent }
\newlabel{aspF}{{2.2}{2}{}{thm.2.2}{}}
\citation{manifold}
\newlabel{secondlip}{{2.8}{3}{}{thm.2.8}{}}
\newlabel{constants}{{2.9}{3}{}{thm.2.9}{}}
\newlabel{ZDG}{{2.10}{3}{}{thm.2.10}{}}
\newlabel{g}{{2.11}{3}{}{thm.2.11}{}}
\newlabel{defg}{{2}{3}{}{equation.2.2}{}}
\citation{dfols}
\citation{dfols}
\@writefile{toc}{\contentsline {section}{\numberline {3}Manifold Sampling}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Major Components}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Derivative Free Model}{4}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Master Model}{4}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{quadratic}{{3}{4}{Master Model}{equation.3.3}{}}
\newlabel{gandd}{{4}{4}{Master Model}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Sufficient Decrease Condition}{4}{subsubsection.3.1.3}\protected@file@percent }
\newlabel{minimize}{{5}{4}{Sufficient Decrease Condition}{equation.3.5}{}}
\newlabel{suffdecrease}{{6}{4}{Sufficient Decrease Condition}{equation.3.6}{}}
\citation{manifold}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Manifold Sampling Loop}{5}{subsubsection.3.1.4}\protected@file@percent }
\newlabel{findz}{{7}{5}{Manifold Sampling Loop}{equation.3.7}{}}
\newlabel{zprime}{{8}{5}{Manifold Sampling Loop}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Test}{5}{subsubsection.3.1.5}\protected@file@percent }
\newlabel{rho}{{9}{5}{Test}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Algorithm}{5}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Some Statement}{5}{subsection.3.3}\protected@file@percent }
\newlabel{etamax}{{10}{5}{Some Statement}{equation.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Theoretical Guarantees}{5}{section.4}\protected@file@percent }
\newlabel{approxco}{{4.1}{5}{}{thm.4.1}{}}
\newlabel{approxg}{{4.2}{6}{}{thm.4.2}{}}
\newlabel{canfindz}{{4.3}{6}{}{thm.4.3}{}}
\newlabel{algorithmcanfindz}{{4.4}{6}{}{thm.4.4}{}}
\newlabel{gets}{{4.5}{6}{}{thm.4.5}{}}
\newlabel{dbdd}{{12}{6}{}{equation.4.12}{}}
\newlabel{mbdd}{{13}{6}{}{equation.4.13}{}}
\newlabel{acciterdecrease}{{4.6}{7}{}{thm.4.6}{}}
\newlabel{iterationdecrease}{{14}{7}{}{equation.4.14}{}}
\citation{manifold}
\citation{benchmark}
\newlabel{deltakbdd}{{15}{8}{}{equation.4.15}{}}
\newlabel{deltatozero}{{4.8}{8}{}{thm.4.8}{}}
\newlabel{liminfg}{{4.9}{8}{}{thm.4.9}{}}
\newlabel{substationary}{{4.10}{8}{}{thm.4.10}{}}
\newlabel{mainthm}{{4.11}{8}{}{thm.4.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulations}{9}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}$L^1$ Norm}{9}{subsection.5.1}\protected@file@percent }
\newlabel{testfunc1}{{16}{9}{$L^1$ Norm}{equation.5.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Function value w.r.t. $x_1$ and $x_2$.\relax }}{9}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:function}{{1}{9}{Function value w.r.t. $x_1$ and $x_2$.\relax }{figure.caption.1}{}}
\citation{dfols}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (Best viewed in color) Distribution of time and returned objective function value. Unsurprisingly, increasing number of points leads to more robust model with trade off in time. \relax }}{10}{figure.caption.2}\protected@file@percent }
\newlabel{fig:npt}{{2}{10}{(Best viewed in color) Distribution of time and returned objective function value. Unsurprisingly, increasing number of points leads to more robust model with trade off in time. \relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (Best viewed in color) Comparison result of three model. Function value is viewed in log scale of difference with oracle obtained by grid search. MF achieves best precision in relatively short time. \relax }}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:compare}{{3}{10}{(Best viewed in color) Comparison result of three model. Function value is viewed in log scale of difference with oracle obtained by grid search. MF achieves best precision in relatively short time. \relax }{figure.caption.3}{}}
\newlabel{testfunc2}{{17}{10}{$L^1$ Norm}{equation.5.17}{}}
\citation{benchmark}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces (Best viewed in color) Test function plotting. Left is plot of manifold around origin, right is contour plot of log value of test function. \relax }}{11}{figure.caption.4}\protected@file@percent }
\newlabel{fig:function2}{{4}{11}{(Best viewed in color) Test function plotting. Left is plot of manifold around origin, right is contour plot of log value of test function. \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Benchmark Problem Set}{11}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Nonconvexity}{11}{subsection.5.3}\protected@file@percent }
\newlabel{testfunc3}{{18}{11}{Nonconvexity}{equation.5.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Experiment result on benchmark. We test MS on nine problem (there was ten problems in benchmark set but problem 7 is not compatible with our formulation by adopting inseparable non-smoothness and compositeness) with input dimension n=2, 5, 10 and 100 to see how MS perform on high dimension tasks. Midway dimension m is intrinsic as illustrate in \ref  {ap3}. We also list out upper bound of $h(L_{\nabla F_i},\cdots  )$ at optimum for further use and we call this "curvature". All results are acquired from single trial but every case is guaranteed not to be outlier by at least five additional trials. By convergence we mean whether $x^k$ and $f(x^k)$ is convergent. By reaching optimum we mean that whether $x^k$ and $f(x^k)$ is approaching the global optimum. For all problems, since function has unique global minimum, we would expect convergence implies reaching optimum. However, violation happened in “Chained CB3 1” and "Chained CB3 2". This is not due to occasional numerical error. We further check that curvature around optimum is relatively large, which we blame this situation on. By error we mean the log scale of difference between optimal value and convergent sequence. Note that for problem "Chained Milfflin", we do not have exact extreme value and thus we use the Cauchy limit of sequence if convergent and omit if not. We also add inferior limits here to illustrate potential convergence.Reason for jumping out of convergence lies in two possibilities. One is numerical explosion and the other is F itself. See below for further discussion. By time and iteration we mean period of time and iterations (less than 3000) spent by algorithm. Positions omitted is where filling this position is meaningless, e.g. time for a non-convergent trial. \relax }}{12}{table.caption.5}\protected@file@percent }
\newlabel{tab:result}{{1}{12}{Experiment result on benchmark. We test MS on nine problem (there was ten problems in benchmark set but problem 7 is not compatible with our formulation by adopting inseparable non-smoothness and compositeness) with input dimension n=2, 5, 10 and 100 to see how MS perform on high dimension tasks. Midway dimension m is intrinsic as illustrate in \ref {ap3}. We also list out upper bound of $h(L_{\nabla F_i},\cdots )$ at optimum for further use and we call this "curvature". All results are acquired from single trial but every case is guaranteed not to be outlier by at least five additional trials. By convergence we mean whether $x^k$ and $f(x^k)$ is convergent. By reaching optimum we mean that whether $x^k$ and $f(x^k)$ is approaching the global optimum. For all problems, since function has unique global minimum, we would expect convergence implies reaching optimum. However, violation happened in “Chained CB3 1” and "Chained CB3 2". This is not due to occasional numerical error. We further check that curvature around optimum is relatively large, which we blame this situation on. By error we mean the log scale of difference between optimal value and convergent sequence. Note that for problem "Chained Milfflin", we do not have exact extreme value and thus we use the Cauchy limit of sequence if convergent and omit if not. We also add inferior limits here to illustrate potential convergence.Reason for jumping out of convergence lies in two possibilities. One is numerical explosion and the other is F itself. See below for further discussion. By time and iteration we mean period of time and iterations (less than 3000) spent by algorithm. Positions omitted is where filling this position is meaningless, e.g. time for a non-convergent trial. \relax }{table.caption.5}{}}
\citation{lars}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces (Best viewed in color) Test function plotting. Left is plot of manifold around origin, right is contour plot of log value of test function. The function has five local minimum, (0,0)(0,1)(0,-1)(2,3)(2,5)\relax }}{13}{figure.caption.6}\protected@file@percent }
\newlabel{fig:function3}{{5}{13}{(Best viewed in color) Test function plotting. Left is plot of manifold around origin, right is contour plot of log value of test function. The function has five local minimum, (0,0)(0,1)(0,-1)(2,3)(2,5)\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (Best viewed in color) We note that in left picture there are many divergent trials still. This is due to searching parameters in \ref  {minimize}. Since 1000 experiments are expensive to run, we could only adopt large search steps, which leads to $x^k$s jumping between two "slope" of loss surface. Given enough time and small search step, then would surely converge.\relax }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{fig:convergence}{{6}{13}{(Best viewed in color) We note that in left picture there are many divergent trials still. This is due to searching parameters in \ref {minimize}. Since 1000 experiments are expensive to run, we could only adopt large search steps, which leads to $x^k$s jumping between two "slope" of loss surface. Given enough time and small search step, then would surely converge.\relax }{figure.caption.7}{}}
\bibstyle{plain}
\bibdata{Proposal.bib}
\bibcite{IoffeAD1979}{1}
\bibcite{distributedcontrol}{2}
\bibcite{MM}{3}
\bibcite{proximal}{4}
\bibcite{CharalambousC}{5}
\bibcite{dfols}{6}
\bibcite{lars}{7}
\bibcite{comp21}{8}
\@writefile{toc}{\contentsline {section}{\numberline {6}Empirical Study}{14}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces (Best viewed in color). Change of loss in log scale through time. MS rapidly finds minimum better than gradient descent. Note that as naive benchmark, we do not perform batch norm or learning rate tuning, which is unfair for MS since no hyper parameter tuning pipeline has been given. Oscillation of MS is due to that parameter set previously is no longer proper after it falls into region with higher "curvature". \relax }}{14}{figure.caption.8}\protected@file@percent }
\newlabel{fig:loss}{{7}{14}{(Best viewed in color). Change of loss in log scale through time. MS rapidly finds minimum better than gradient descent. Note that as naive benchmark, we do not perform batch norm or learning rate tuning, which is unfair for MS since no hyper parameter tuning pipeline has been given. Oscillation of MS is due to that parameter set previously is no longer proper after it falls into region with higher "curvature". \relax }{figure.caption.8}{}}
\bibcite{comp22}{9}
\bibcite{NonlinearEqLInfty}{10}
\bibcite{bundle2}{11}
\bibcite{bundlefree1}{12}
\bibcite{benchmark}{13}
\bibcite{NonlinearEqLp}{14}
\bibcite{proximalauglag}{15}
\bibcite{manifoldpiecewiselinear}{16}
\bibcite{grasam2}{17}
\bibcite{manifoldl1}{18}
\bibcite{comp44}{19}
\bibcite{grasam1}{20}
\bibcite{ADMM}{21}
\bibcite{bundlefree2}{22}
\bibcite{phaseretrieval}{23}
\bibcite{accproximal}{24}
\bibcite{Lasso}{25}
\bibcite{ElasticNet}{26}
\bibcite{BerkeJV1987}{27}
\bibcite{comp55}{28}
\bibcite{comp56}{29}
\bibcite{Lagrange}{30}
\bibcite{lowcomplexity}{31}
\bibcite{manifold}{32}
\bibcite{bundle3}{33}
\bibcite{bundle}{34}
\@writefile{toc}{\contentsline {section}{\numberline {A}Code}{17}{appendix.A}\protected@file@percent }
\newlabel{ap1}{{A}{17}{Code}{appendix.A}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Manifold Sampling\relax }}{17}{algorithm.1}\protected@file@percent }
\newlabel{algorithm1}{{1}{17}{Manifold Sampling\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Test Performance}{17}{appendix.B}\protected@file@percent }
\newlabel{ap2}{{B}{17}{Test Performance}{appendix.B}{}}
\citation{benchmark}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Grid Search for z and j\relax }}{18}{algorithm.2}\protected@file@percent }
\newlabel{algorithm2}{{2}{18}{Grid Search for z and j\relax }{algorithm.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces (Best viewed in color) Distribution of time and returned objective function value. There is one trial for npt=14 that broke down, which led to poor performance. \relax }}{18}{figure.caption.10}\protected@file@percent }
\newlabel{fig:npt2}{{8}{18}{(Best viewed in color) Distribution of time and returned objective function value. There is one trial for npt=14 that broke down, which led to poor performance. \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces (Best viewed in color) Comparison result of three model. Function value is viewed in log scale of difference with oracle obtained by grid search. MF achieves best precision in relatively short time. \relax }}{18}{figure.caption.11}\protected@file@percent }
\newlabel{fig:compare2}{{9}{18}{(Best viewed in color) Comparison result of three model. Function value is viewed in log scale of difference with oracle obtained by grid search. MF achieves best precision in relatively short time. \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Benchmark Problem Set}{19}{appendix.C}\protected@file@percent }
\newlabel{ap3}{{C}{19}{Benchmark Problem Set}{appendix.C}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Details of test problem set. \relax }}{20}{table.caption.12}\protected@file@percent }
\newlabel{tab:test}{{2}{20}{Details of test problem set. \relax }{table.caption.12}{}}
